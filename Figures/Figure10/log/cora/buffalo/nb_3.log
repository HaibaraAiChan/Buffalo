main start at this time 1733007526.4600935
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006593227386474609
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  0.000415802001953125
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0012161731719970703
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.007067441940307617
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.009623527526855469
self.buckets_partition() spend  sec:  0.008295774459838867
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 0.90771484375 GB
    Memory Allocated: 0.37457942962646484  GigaBytes
Max Memory Allocated: 0.37457942962646484  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.39404296875 GB
    Memory Allocated: 0.8171415328979492  GigaBytes
Max Memory Allocated: 0.8187031745910645  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.39404296875 GB
    Memory Allocated: 0.817143440246582  GigaBytes
Max Memory Allocated: 0.8187031745910645  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.83935546875 GB
    Memory Allocated: 0.7565598487854004  GigaBytes
Max Memory Allocated: 1.2028899192810059  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.83935546875 GB
    Memory Allocated: 1.2283344268798828  GigaBytes
Max Memory Allocated: 1.2307958602905273  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.83935546875 GB
    Memory Allocated: 1.2283368110656738  GigaBytes
Max Memory Allocated: 1.2307958602905273  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.08935546875 GB
    Memory Allocated: 0.7560272216796875  GigaBytes
Max Memory Allocated: 1.4730849266052246  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.08935546875 GB
    Memory Allocated: 1.1957168579101562  GigaBytes
Max Memory Allocated: 1.4730849266052246  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.08935546875 GB
    Memory Allocated: 1.1957182884216309  GigaBytes
Max Memory Allocated: 1.4730849266052246  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.1200041770935059  GigaBytes
Max Memory Allocated: 1.851142406463623  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7131720781326294
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.0065572261810302734
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.653236389160156e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0009005069732666016
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006677865982055664
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00852823257446289
self.buckets_partition() spend  sec:  0.007586956024169922
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 1.851142406463623  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.5533218383789062  GigaBytes
Max Memory Allocated: 1.851142406463623  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.55328369140625  GigaBytes
Max Memory Allocated: 1.851142406463623  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 1.9311084747314453  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.963017463684082  GigaBytes
Max Memory Allocated: 1.9654788970947266  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72998046875 GB
    Memory Allocated: 1.963019847869873  GigaBytes
Max Memory Allocated: 1.9654788970947266  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.484537124633789  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9306831359863281  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9306845664978027  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1200041770935059  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7297015190124512
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006519794464111328
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.510185241699219e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008420944213867188
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006646394729614258
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00843954086303711
self.buckets_partition() spend  sec:  0.007497549057006836
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5536155700683594  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5535774230957031  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9567079544067383  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9567103385925293  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.4845361709594727  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9228239059448242  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9228253364562988  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.120002269744873  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7185122966766357
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006494283676147461
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.486343383789062e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008304119110107422
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0066127777099609375
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008362054824829102
self.buckets_partition() spend  sec:  0.007452726364135742
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1196842193603516  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.55615234375  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5561141967773438  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9617133140563965  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9617156982421875  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.484424114227295  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9183931350708008  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9183945655822754  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1198902130126953  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6342405080795288
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006583213806152344
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.414817810058594e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008234977722167969
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006698131561279297
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008432149887084961
self.buckets_partition() spend  sec:  0.007530689239501953
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.555528163909912  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5554900169372559  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9621868133544922  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9621891975402832  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.4844346046447754  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9225263595581055  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.92252779006958  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1199007034301758  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.587454080581665
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006537437438964844
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.581710815429688e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00081634521484375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006653547286987305
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00838613510131836
self.buckets_partition() spend  sec:  0.0074787139892578125
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1196789741516113  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.554046630859375  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5540084838867188  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9664463996887207  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9664487838745117  GigaBytes
Max Memory Allocated: 2.207767963409424  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.4846062660217285  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.922830581665039  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9228320121765137  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1200733184814453  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.523370385169983
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.0065004825592041016
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.343292236328125e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008313655853271484
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006615638732910156
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008368253707885742
self.buckets_partition() spend  sec:  0.007454872131347656
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1197218894958496  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5581073760986328  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5580692291259766  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9617295265197754  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9617319107055664  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.484632968902588  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9276175498962402  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9276189804077148  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1201000213623047  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.433156967163086
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006495952606201172
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.367134094238281e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008237361907958984
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0066089630126953125
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00836181640625
self.buckets_partition() spend  sec:  0.007441282272338867
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1197004318237305  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5598316192626953  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.559793472290039  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9575285911560059  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9575309753417969  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.484391689300537  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9235243797302246  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.9235258102416992  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1198577880859375  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6124876737594604
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.0064699649810791016
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  6.985664367675781e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008833408355712891
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00657963752746582
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008391380310058594
self.buckets_partition() spend  sec:  0.007471323013305664
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.1198339462280273  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.5592021942138672  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85498046875 GB
    Memory Allocated: 1.559164047241211  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9593505859375  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.959352970123291  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.484499454498291  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9295659065246582  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9295673370361328  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1199665069580078  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8070478439331055
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006547451019287109
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  8.392333984375e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0009036064147949219
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006685495376586914
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008537769317626953
self.buckets_partition() spend  sec:  0.007601022720336914
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196622848510742  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.55348539352417  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5534472465515137  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9615821838378906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9615845680236816  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.48451566696167  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9315309524536133  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.931532382965088  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1199827194213867  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.582025408744812
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.0065402984619140625
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.081031799316406e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008902549743652344
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006650686264038086
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008459806442260742
self.buckets_partition() spend  sec:  0.0075495243072509766
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5532102584838867  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5531721115112305  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485666275024414  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.962052345275879  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.96205472946167  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4844303131103516  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9264569282531738  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9264583587646484  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1198973655700684  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3729989528656006
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006496906280517578
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.343292236328125e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008299350738525391
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006632089614868164
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008394002914428711
self.buckets_partition() spend  sec:  0.007470607757568359
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5540571212768555  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5540189743041992  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9619736671447754  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9619760513305664  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.48451566696167  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.92356538772583  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9235668182373047  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1199827194213867  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1685835123062134
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006528615951538086
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.033348083496094e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008261203765869141
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006693601608276367
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008501768112182617
self.buckets_partition() spend  sec:  0.007528543472290039
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1197056770324707  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5553045272827148  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5552663803100586  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9580583572387695  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9580607414245605  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4844293594360352  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.918172836303711  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9181742668151855  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1198954582214355  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.0470668077468872
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006500720977783203
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.43865966796875e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008258819580078125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00661468505859375
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008347749710083008
self.buckets_partition() spend  sec:  0.007449626922607422
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1197166442871094  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5596561431884766  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5596179962158203  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9576668739318848  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9576692581176758  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4845304489135742  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9197425842285156  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9197440147399902  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1199965476989746  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9586688280105591
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006607532501220703
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.009506225585938e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008206367492675781
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0067195892333984375
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008463382720947266
self.buckets_partition() spend  sec:  0.007549285888671875
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5575203895568848  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5574822425842285  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9632830619812012  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9632854461669922  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4845466613769531  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9227471351623535  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9227485656738281  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1200127601623535  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7866610288619995
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006483316421508789
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  6.842613220214844e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008258819580078125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006596803665161133
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008348464965820312
self.buckets_partition() spend  sec:  0.0074312686920166016
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.119673728942871  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5563287734985352  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.556290626525879  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9574613571166992  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9574637413024902  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.484499454498291  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.922698974609375  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9227004051208496  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1199665069580078  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6161440014839172
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006581544876098633
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  6.961822509765625e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008246898651123047
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006693124771118164
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00843667984008789
self.buckets_partition() spend  sec:  0.0075266361236572266
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5572853088378906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5572471618652344  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.959916591644287  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9599189758300781  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.484574317932129  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9245734214782715  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.924574851989746  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1200413703918457  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6357954144477844
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006516933441162109
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.414817810058594e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008263587951660156
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006630897521972656
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008368730545043945
self.buckets_partition() spend  sec:  0.0074656009674072266
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.119710922241211  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5557212829589844  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5556831359863281  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9561357498168945  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9561381340026855  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4843754768371582  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.918729305267334  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9187307357788086  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1198415756225586  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5624983310699463
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006540536880493164
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.700920104980469e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008404254913330078
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006736278533935547
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00850057601928711
self.buckets_partition() spend  sec:  0.007588624954223633
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1196632385253906  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5568485260009766  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5568103790283203  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9607577323913574  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9607601165771484  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.48441743850708  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.915132999420166  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9151344299316406  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.1198816299438477  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9810794591903687
the output layer 
self.num_batch (get_in_degree_bucketing) 3
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  3
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  3
G_BUCKET_ID_list [[5, 8, 6, 9], [3, 2, 0], [1, 14, 7, 11, 12], [4, 13, 10]]
Groups_mem_list  [[232, 151, 116, 101], [307, 224, 69], [189, 185, 94, 80, 51], [277, 73, 36]]
G_BUCKET_ID_list length 4
backpack scheduling spend  0.006541252136230469
current group_mem  0.6024863719940186
current group_mem  0.6019114404916763
current group_mem  0.6002907156944275
current group_mem  0.38802625983953476
batches output list generation spend  7.319450378417969e-05
self.weights_list  [0.15714285714285714, 0.4857142857142857, 0.2357142857142857, 0.12142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008263587951660156
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00665593147277832
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008393049240112305
self.buckets_partition() spend  sec:  0.007491350173950195
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.119676113128662  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5549354553222656  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.5549001693725586  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.485766887664795  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9628748893737793  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9628772735595703  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.4844560623168945  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9257707595825195  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.9257721900939941  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.85693359375 GB
    Memory Allocated: 1.119922161102295  GigaBytes
Max Memory Allocated: 2.2111968994140625  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.4262985289096832
epoch_time_list  [4.194265127182007, 1.3829915523529053, 1.3753526210784912, 1.346259593963623, 1.3722331523895264, 1.3770999908447266, 1.380418300628662, 1.3765995502471924, 1.3794779777526855, 1.377800703048706, 1.3769166469573975, 1.3755757808685303, 1.3464524745941162, 1.3419528007507324, 1.3769688606262207, 1.378455638885498, 1.3508291244506836, 1.3789129257202148, 1.3490185737609863, 1.3784458637237549]

loading_time list   [0.0047185420989990234, 0.005650043487548828, 0.004301786422729492, 0.004226207733154297, 0.004632472991943359, 0.004603862762451172, 0.004836320877075195, 0.00426483154296875, 0.004301548004150391, 0.0049173831939697266, 0.004217624664306641, 0.0043027400970458984, 0.00424504280090332, 0.004595756530761719, 0.004584312438964844, 0.004256010055541992, 0.00427556037902832, 0.0048007965087890625, 0.004556179046630859, 0.004749298095703125]

 data loader gen time  [0.04914045333862305, 0.04781770706176758, 0.04626727104187012, 0.046828508377075195, 0.04483962059020996, 0.04515218734741211, 0.04737377166748047, 0.046346426010131836, 0.0456233024597168, 0.046202659606933594, 0.04558992385864258, 0.045839786529541016, 0.0475916862487793, 0.04542183876037598, 0.046868324279785156, 0.0467069149017334, 0.047611236572265625, 0.048076629638671875, 0.04926753044128418, 0.044615745544433594]
	---backpack schedule time  [0.00994563102722168, 0.008826971054077148, 0.00871586799621582, 0.008630514144897461, 0.00869607925415039, 0.008648395538330078, 0.008628606796264648, 0.008622407913208008, 0.008671045303344727, 0.008846759796142578, 0.008723735809326172, 0.008664131164550781, 0.008772134780883789, 0.008613109588623047, 0.008721590042114258, 0.008629322052001953, 0.008697271347045898, 0.00863504409790039, 0.008792400360107422, 0.008656740188598633]
	---connection_check_time_list  [0.018599510192871094, 0.018646240234375, 0.01809096336364746, 0.017882585525512695, 0.017802000045776367, 0.017720699310302734, 0.018229961395263672, 0.017792463302612305, 0.017623186111450195, 0.01802349090576172, 0.017951250076293945, 0.017841339111328125, 0.018293142318725586, 0.017786026000976562, 0.018151044845581055, 0.018041610717773438, 0.0182037353515625, 0.018232107162475586, 0.018364429473876953, 0.017713546752929688]
	---block_gen_time_list  [0.018435001373291016, 0.018063783645629883, 0.017234086990356445, 0.01809525489807129, 0.016147136688232422, 0.016582489013671875, 0.018148183822631836, 0.017751693725585938, 0.017120838165283203, 0.017075300216674805, 0.016723155975341797, 0.016986370086669922, 0.01829838752746582, 0.0168304443359375, 0.01776885986328125, 0.01782965660095215, 0.018488407135009766, 0.01897120475769043, 0.019853830337524414, 0.016051054000854492]
training time  [4.14040207862854, 1.3277499675750732, 1.3232266902923584, 1.2937138080596924, 1.3212850093841553, 1.3259022235870361, 1.3266465663909912, 1.3240303993225098, 1.327836513519287, 1.3251781463623047, 1.3253228664398193, 1.3234033584594727, 1.2927682399749756, 1.2904932498931885, 1.3240678310394287, 1.3255984783172607, 1.2970921993255615, 1.3244848251342773, 1.2932970523834229, 1.327559232711792]
---feature block loading time  [0.1789085865020752, 0.1755986213684082, 0.17505669593811035, 0.1752455234527588, 0.17507076263427734, 0.17728757858276367, 0.17622947692871094, 0.17496180534362793, 0.17539095878601074, 0.1749119758605957, 0.17505168914794922, 0.17442584037780762, 0.1752176284790039, 0.17530536651611328, 0.17403483390808105, 0.1751723289489746, 0.17579174041748047, 0.174576997756958, 0.17548251152038574, 0.17583894729614258]


epoch_time avg   1.3698223978281021
loading_time avg   0.0045087337493896484
 data loader gen time avg 0.04644547402858734
	---backpack schedule time avg 0.008688673377037048
	---connection_check_time avg  0.017985627055168152
	---block_gen_time avg  0.017539188265800476
training time  1.3171853870153427
---feature block loading time  0.17529690265655518
pure train time per /epoch  [3.953744411468506, 1.0581166744232178, 1.0538592338562012, 1.024656057357788, 1.053398847579956, 1.0554890632629395, 1.0561697483062744, 1.0547187328338623, 1.059077501296997, 1.0560524463653564, 1.056410312652588, 1.0554609298706055, 1.0241131782531738, 1.0219616889953613, 1.056302785873413, 1.0562639236450195, 1.0274534225463867, 1.0559046268463135, 1.0246059894561768, 1.0579447746276855]
pure train time average  1.0468225899864645

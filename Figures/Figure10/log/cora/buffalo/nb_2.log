main start at this time 1733007493.610886
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005814790725708008
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  0.0003731250762939453
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0015883445739746094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006233930587768555
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00869607925415039
self.buckets_partition() spend  sec:  0.007834434509277344
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 0.90771484375 GB
    Memory Allocated: 0.3768177032470703  GigaBytes
Max Memory Allocated: 0.3768177032470703  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.64013671875 GB
    Memory Allocated: 1.0648632049560547  GigaBytes
Max Memory Allocated: 1.0686078071594238  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.64013671875 GB
    Memory Allocated: 1.064866542816162  GigaBytes
Max Memory Allocated: 1.0686078071594238  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.08544921875 GB
    Memory Allocated: 0.7582669258117676  GigaBytes
Max Memory Allocated: 1.4247698783874512  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.10302734375 GB
    Memory Allocated: 1.4565072059631348  GigaBytes
Max Memory Allocated: 1.459853172302246  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.10302734375 GB
    Memory Allocated: 1.4565095901489258  GigaBytes
Max Memory Allocated: 1.459853172302246  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.94873046875 GB
    Memory Allocated: 1.122847080230713  GigaBytes
Max Memory Allocated: 1.8534111976623535  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9482998847961426
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.0058557987213134766
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.792213439941406e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006921291351318359
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005944967269897461
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0071561336517333984
self.buckets_partition() spend  sec:  0.006646633148193359
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.94873046875 GB
    Memory Allocated: 1.12330961227417  GigaBytes
Max Memory Allocated: 1.8534111976623535  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.95263671875 GB
    Memory Allocated: 1.8074746131896973  GigaBytes
Max Memory Allocated: 1.8534111976623535  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.95263671875 GB
    Memory Allocated: 1.80741548538208  GigaBytes
Max Memory Allocated: 1.8534111976623535  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.95263671875 GB
    Memory Allocated: 1.4880013465881348  GigaBytes
Max Memory Allocated: 2.1593117713928223  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.95263671875 GB
    Memory Allocated: 2.187713146209717  GigaBytes
Max Memory Allocated: 2.191059112548828  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.95263671875 GB
    Memory Allocated: 2.187715530395508  GigaBytes
Max Memory Allocated: 2.191059112548828  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122847080230713  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.0069966316223145
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005887269973754883
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  9.751319885253906e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006325244903564453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.006020069122314453
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007133007049560547
self.buckets_partition() spend  sec:  0.006661653518676758
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.12330961227417  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8047032356262207  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8046441078186035  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4886517524719238  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1875052452087402  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1875076293945312  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122847080230713  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9712886810302734
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.0058329105377197266
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.38690185546875e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006687641143798828
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005911827087402344
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007083892822265625
self.buckets_partition() spend  sec:  0.00658869743347168
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1233086585998535  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.80519437789917  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8051352500915527  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4880084991455078  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1919775009155273  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1919798851013184  GigaBytes
Max Memory Allocated: 2.4350500106811523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122854232788086  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8858580589294434
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005814790725708008
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.4809112548828125e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006322860717773438
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005891084671020508
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.006991863250732422
self.buckets_partition() spend  sec:  0.006531953811645508
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122591495513916  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8084220886230469  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8083629608154297  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4894418716430664  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1930627822875977  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1930651664733887  GigaBytes
Max Memory Allocated: 2.439314365386963  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235651969909668  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8405370712280273
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.00582432746887207
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.790855407714844e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006663799285888672
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0059015750885009766
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007071018218994141
self.buckets_partition() spend  sec:  0.006577491760253906
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226978302001953  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.810102939605713  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8100438117980957  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.488114356994629  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1932425498962402  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1932449340820312  GigaBytes
Max Memory Allocated: 2.440399646759033  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1229610443115234  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7895206212997437
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005803823471069336
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.4332275390625e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006275177001953125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0058689117431640625
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.006974458694458008
self.buckets_partition() spend  sec:  0.006505012512207031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226234436035156  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8054747581481934  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8054156303405762  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4880013465881348  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.194540023803711  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.194542407989502  GigaBytes
Max Memory Allocated: 2.440579414367676  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122847080230713  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6549782752990723
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005880832672119141
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.62396240234375e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006327629089355469
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005949735641479492
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007123231887817383
self.buckets_partition() spend  sec:  0.006650209426879883
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1233086585998535  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.798220157623291  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7981610298156738  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4880061149597168  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188133716583252  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188136100769043  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.122851848602295  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6125054359436035
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005840301513671875
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.457069396972656e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006225109100341797
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005908966064453125
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007010459899902344
self.buckets_partition() spend  sec:  0.006544351577758789
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.805624008178711  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8055648803710938  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4893059730529785  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188249111175537  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188251495361328  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235013008117676  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.8055367469787598
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005894660949707031
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.457069396972656e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006358623504638672
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005960941314697266
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007070302963256836
self.buckets_partition() spend  sec:  0.006609201431274414
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8130578994750977  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8129987716674805  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4893059730529785  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1915159225463867  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1915183067321777  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235013008117676  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.8161243200302124
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005801677703857422
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.814697265625e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006363391876220703
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005871295928955078
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.006975650787353516
self.buckets_partition() spend  sec:  0.006516456604003906
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8003182411193848  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8002591133117676  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4886550903320312  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.19326114654541  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.193263530731201  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235008239746094  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.46612548828125
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.00590825080871582
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.315376281738281e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006449222564697266
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005987882614135742
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0071277618408203125
self.buckets_partition() spend  sec:  0.006643772125244141
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8046293258666992  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.804570198059082  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4892354011535645  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1905322074890137  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1905345916748047  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235013008117676  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4300172328948975
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005817413330078125
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.62396240234375e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006244182586669922
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005889177322387695
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0070438385009765625
self.buckets_partition() spend  sec:  0.006522655487060547
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226544380187988  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8023643493652344  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8023052215576172  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.488652229309082  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1856894493103027  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1856918334960938  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1234979629516602  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3473728895187378
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005824565887451172
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.790855407714844e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006668567657470703
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0059015750885009766
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007038116455078125
self.buckets_partition() spend  sec:  0.006577014923095703
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226544380187988  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.799799919128418  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7997407913208008  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.488652229309082  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1935439109802246  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1935462951660156  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1234979629516602  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2605308294296265
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.0058650970458984375
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.9577484130859375e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006766319274902344
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005940675735473633
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007134914398193359
self.buckets_partition() spend  sec:  0.0066258907318115234
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8035988807678223  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.803539752960205  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4886550903320312  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1873178482055664  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1873202323913574  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235008239746094  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1009242534637451
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005825042724609375
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.029273986816406e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006215572357177734
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005898475646972656
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0069959163665771484
self.buckets_partition() spend  sec:  0.006529569625854492
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8033580780029297  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8032989501953125  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4886555671691895  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188100814819336  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.188103199005127  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235013008117676  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9790126085281372
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.00584721565246582
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.315376281738281e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0007653236389160156
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005931377410888672
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007238864898681641
self.buckets_partition() spend  sec:  0.006704568862915039
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226544380187988  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8045506477355957  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8044915199279785  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.4893031120300293  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1932802200317383  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1932826042175293  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1234984397888184  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7866024374961853
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005843639373779297
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.62396240234375e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006461143493652344
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00591588020324707
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007033824920654297
self.buckets_partition() spend  sec:  0.006570100784301758
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226534843444824  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7947335243225098  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7946743965148926  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.488652229309082  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1867079734802246  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1867103576660156  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1234979629516602  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6645631194114685
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.005869865417480469
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  4.124641418457031e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006334781646728516
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0059473514556884766
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007054805755615234
self.buckets_partition() spend  sec:  0.006589174270629883
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226544380187988  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7942519187927246  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.7941927909851074  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.488652229309082  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1870288848876953  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.1870312690734863  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1234979629516602  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.5439158082008362
the output layer 
self.num_batch (get_in_degree_bucketing) 2
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  2
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
sum(estimated_mem)
2.192714788019657
15
self.K  2
G_BUCKET_ID_list [[4, 2, 1, 14, 8, 7], [3, 5, 6, 9, 11, 13, 0, 12, 10]]
Groups_mem_list  [[277, 224, 189, 185, 151, 94], [307, 232, 116, 101, 80, 73, 69, 51, 36]]
G_BUCKET_ID_list length 2
backpack scheduling spend  0.00587010383605957
current group_mem  1.1224348694086075
current group_mem  1.0702799186110497
batches output list generation spend  3.600120544433594e-05
self.weights_list  [0.5357142857142857, 0.4642857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006272792816162109
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005948066711425781
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007046937942504883
self.buckets_partition() spend  sec:  0.006583452224731445
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1226553916931152  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8007259368896484  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.8006668090820312  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.489389419555664  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.193727493286133  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 2.193729877471924  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 3.07763671875 GB
    Memory Allocated: 1.1235008239746094  GigaBytes
Max Memory Allocated: 2.4418768882751465  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.43811172246932983
epoch_time_list  [2.397639274597168, 1.1993381977081299, 1.1978693008422852, 1.164808750152588, 1.1982073783874512, 1.1963207721710205, 1.1947431564331055, 1.1970124244689941, 1.215362787246704, 1.196028709411621, 1.1967802047729492, 1.1955833435058594, 1.16389799118042, 1.173018455505371, 1.1962716579437256, 1.1989009380340576, 1.1666772365570068, 1.1959192752838135, 1.165822982788086, 1.1982429027557373]

loading_time list   [0.0042726993560791016, 0.004765748977661133, 0.003724813461303711, 0.0038449764251708984, 0.003709554672241211, 0.003873109817504883, 0.0036640167236328125, 0.003641366958618164, 0.0038726329803466797, 0.003679037094116211, 0.0034368038177490234, 0.0035047531127929688, 0.0034089088439941406, 0.0037109851837158203, 0.0040378570556640625, 0.0036597251892089844, 0.0036940574645996094, 0.0037360191345214844, 0.0034389495849609375, 0.0036818981170654297]

 data loader gen time  [0.0341031551361084, 0.030905961990356445, 0.03185129165649414, 0.031058549880981445, 0.03267621994018555, 0.0307004451751709, 0.03101968765258789, 0.030852317810058594, 0.04165005683898926, 0.03157210350036621, 0.030724525451660156, 0.030522584915161133, 0.030866622924804688, 0.03273153305053711, 0.030951976776123047, 0.03380465507507324, 0.03268074989318848, 0.03141522407531738, 0.03149700164794922, 0.030282258987426758]
	---backpack schedule time  [0.008987665176391602, 0.0074214935302734375, 0.007368326187133789, 0.007334232330322266, 0.00722813606262207, 0.0073146820068359375, 0.007212400436401367, 0.00735163688659668, 0.007241964340209961, 0.007303953170776367, 0.007210254669189453, 0.0073816776275634766, 0.007276773452758789, 0.007286548614501953, 0.007390499114990234, 0.007234096527099609, 0.0075342655181884766, 0.0072705745697021484, 0.007293224334716797, 0.007283449172973633]
	---connection_check_time_list  [0.011397123336791992, 0.011109590530395508, 0.011211633682250977, 0.011505603790283203, 0.01182103157043457, 0.011239767074584961, 0.01121973991394043, 0.01089024543762207, 0.016900062561035156, 0.011369705200195312, 0.010952949523925781, 0.01138615608215332, 0.010978221893310547, 0.011421680450439453, 0.011260509490966797, 0.015160799026489258, 0.012241363525390625, 0.011446475982666016, 0.011516571044921875, 0.011345386505126953]
	---block_gen_time_list  [0.011921882629394531, 0.010529518127441406, 0.011302709579467773, 0.010397911071777344, 0.011718273162841797, 0.010319709777832031, 0.010734796524047852, 0.010730743408203125, 0.014430761337280273, 0.011034011840820312, 0.010722637176513672, 0.009899139404296875, 0.010763168334960938, 0.01194620132446289, 0.010443925857543945, 0.009587526321411133, 0.01091313362121582, 0.010733604431152344, 0.010793447494506836, 0.009843587875366211]
training time  [2.359260082244873, 1.1626229286193848, 1.1614468097686768, 1.1290643215179443, 1.1610257625579834, 1.160797119140625, 1.159212589263916, 1.161665439605713, 1.1689503192901611, 1.159963846206665, 1.1616020202636719, 1.160508155822754, 1.1285545825958252, 1.1356828212738037, 1.160447359085083, 1.160597562789917, 1.1294658184051514, 1.1599524021148682, 1.1298301219940186, 1.1634721755981445]
---feature block loading time  [0.10273241996765137, 0.09775781631469727, 0.09787845611572266, 0.09705781936645508, 0.09772181510925293, 0.09696197509765625, 0.09707188606262207, 0.09731626510620117, 0.09745430946350098, 0.09698247909545898, 0.09697794914245605, 0.09802865982055664, 0.09694385528564453, 0.09945821762084961, 0.09723877906799316, 0.09706997871398926, 0.09715890884399414, 0.09706902503967285, 0.09726428985595703, 0.09715795516967773]


epoch_time avg   1.1905493885278702
loading_time avg   0.0036718547344207764
 data loader gen time avg 0.032121747732162476
	---backpack schedule time avg 0.007300883531570435
	---connection_check_time avg  0.011946916580200195
	---block_gen_time avg  0.010913416743278503
training time  1.1538580060005188
---feature block loading time  0.09736727178096771
pure train time per /epoch  [2.2495601177215576, 0.9623391628265381, 0.9613134860992432, 0.9296407699584961, 0.9608182907104492, 0.9611740112304688, 0.959740400314331, 0.9613871574401855, 0.9688727855682373, 0.9604897499084473, 0.9617235660552979, 0.9600248336791992, 0.9294466972351074, 0.933107852935791, 0.9604136943817139, 0.9618415832519531, 0.9296364784240723, 0.9600002765655518, 0.9298033714294434, 0.9632613658905029]
pure train time average  0.9524342873517204

main start at this time 1733007659.592061
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.00505828857421875
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  0.00045013427734375
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008840560913085938
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0055654048919677734
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00834798812866211
self.buckets_partition() spend  sec:  0.00646209716796875
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 0.90771484375 GB
    Memory Allocated: 0.3721475601196289  GigaBytes
Max Memory Allocated: 0.3721475601196289  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.21435546875 GB
    Memory Allocated: 0.6511316299438477  GigaBytes
Max Memory Allocated: 0.6523571014404297  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.21435546875 GB
    Memory Allocated: 0.6511330604553223  GigaBytes
Max Memory Allocated: 0.6523571014404297  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.67919921875 GB
    Memory Allocated: 0.7542486190795898  GigaBytes
Max Memory Allocated: 1.048100471496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.68505859375 GB
    Memory Allocated: 1.0363311767578125  GigaBytes
Max Memory Allocated: 1.048100471496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.68505859375 GB
    Memory Allocated: 1.036332607269287  GigaBytes
Max Memory Allocated: 1.048100471496582  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 0.7546243667602539  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0121006965637207  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0121021270751953  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 0.7542901039123535  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0044760704040527  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0044775009155273  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 0.754664421081543  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0136065483093262  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0136079788208008  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 0.754331111907959  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0042376518249512  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.87255859375 GB
    Memory Allocated: 1.0042386054992676  GigaBytes
Max Memory Allocated: 1.281890869140625  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.118283748626709  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.695178747177124
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0050122737884521484
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  0.0003154277801513672
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.000911712646484375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005383729934692383
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00800013542175293
self.buckets_partition() spend  sec:  0.0063076019287109375
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.1172375679016113  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.3870291709899902  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.387010097503662  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.4825878143310547  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7655830383300781  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7655844688415527  GigaBytes
Max Memory Allocated: 1.8509936332702637  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.4827003479003906  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7427778244018555  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.74277925491333  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.4821434020996094  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7354326248168945  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7354340553283691  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.482469081878662  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7463288307189941  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7463302612304688  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.482088565826416  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7325916290283203  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.63037109375 GB
    Memory Allocated: 1.7325925827026367  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.69287109375 GB
    Memory Allocated: 1.1175312995910645  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6696511507034302
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0049114227294921875
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.988380432128906e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006422996520996094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0050504207611083984
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007164478302001953
self.buckets_partition() spend  sec:  0.005700588226318359
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.69287109375 GB
    Memory Allocated: 1.1172375679016113  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.69287109375 GB
    Memory Allocated: 1.3880982398986816  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.69287109375 GB
    Memory Allocated: 1.3880791664123535  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.69287109375 GB
    Memory Allocated: 1.4825773239135742  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.760056495666504  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7600579261779785  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.482865810394287  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7436847686767578  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7436861991882324  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4821105003356934  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7324872016906738  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7324886322021484  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4822769165039062  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7443366050720215  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.744338035583496  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4820990562438965  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.731287956237793  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7312889099121094  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.1175427436828613  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7686119079589844
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0049703121185302734
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  0.0001201629638671875
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0008401870727539062
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005137443542480469
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0076258182525634766
self.buckets_partition() spend  sec:  0.00598597526550293
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.1172370910644531  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3858776092529297  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3858580589294434  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.482694149017334  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7660284042358398  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7660298347473145  GigaBytes
Max Memory Allocated: 2.0111427307128906  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4827799797058105  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7401823997497559  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7401838302612305  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4821858406066895  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7366700172424316  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7366714477539062  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4823665618896484  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7434511184692383  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.743452548980713  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4821572303771973  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7354474067687988  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7354483604431152  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.117600917816162  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6152211427688599
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0049097537994384766
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.249282836914062e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006411075592041016
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00503230094909668
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.00709986686706543
self.buckets_partition() spend  sec:  0.0056819915771484375
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.1172380447387695  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3904390335083008  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3904194831848145  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4826679229736328  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.765225887298584  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7652273178100586  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.482785701751709  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7427434921264648  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7427449226379395  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4822020530700684  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7337045669555664  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.733705997467041  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4824156761169434  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7420964241027832  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7420978546142578  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4820404052734375  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7327170372009277  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.7327179908752441  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.1174840927124023  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6095349788665771
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004951953887939453
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.368492126464844e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006282329559326172
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005077838897705078
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0071294307708740234
self.buckets_partition() spend  sec:  0.005713701248168945
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.1173081398010254  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3912663459777832  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.3912467956542969  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.71826171875 GB
    Memory Allocated: 1.4826784133911133  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.765310287475586  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7653117179870605  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828286170959473  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7414665222167969  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7414679527282715  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482255458831787  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7361869812011719  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7361884117126465  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825010299682617  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7515535354614258  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7515549659729004  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482109546661377  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734457015991211  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7344579696655273  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175532341003418  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5138413906097412
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004927635192871094
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.775161743164062e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006585121154785156
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0050754547119140625
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007187604904174805
self.buckets_partition() spend  sec:  0.005745649337768555
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1173505783081055  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.394446849822998  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3944272994995117  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4832568168640137  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7627511024475098  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7627525329589844  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4835829734802246  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7438411712646484  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.743842601776123  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828338623046875  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7345094680786133  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734510898590088  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4830689430236816  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7454524040222168  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7454538345336914  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828376770019531  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7322349548339844  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7322359085083008  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1176176071166992  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4511241912841797
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004912137985229492
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.179115295410156e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006341934204101562
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005045890808105469
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007124900817871094
self.buckets_partition() spend  sec:  0.005689859390258789
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172370910644531  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.38496732711792  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3849477767944336  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825773239135742  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7612833976745605  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7612848281860352  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4827003479003906  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7383036613464355  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7383050918579102  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482132911682129  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734694004058838  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7346954345703125  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482367992401123  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.744722843170166  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7447242736816406  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4821696281433105  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7328910827636719  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7328920364379883  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1176142692565918  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.333878993988037
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0049326419830322266
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.012222290039062e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006415843963623047
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00507044792175293
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007158756256103516
self.buckets_partition() spend  sec:  0.005721092224121094
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172761917114258  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3917183876037598  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3916988372802734  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4831900596618652  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7636809349060059  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7636823654174805  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4833831787109375  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7433061599731445  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7433075904846191  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826765060424805  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7338008880615234  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.733802318572998  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482980728149414  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7482876777648926  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7482891082763672  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482642650604248  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7317118644714355  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.731712818145752  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1176323890686035  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3407729864120483
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004904031753540039
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.344650268554688e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006914138793945312
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005028963088989258
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007148027420043945
self.buckets_partition() spend  sec:  0.005728721618652344
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172699928283691  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3953957557678223  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3953766822814941  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825396537780762  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7610135078430176  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7610149383544922  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482823371887207  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7456746101379395  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.745676040649414  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4822287559509277  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7340521812438965  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734053611755371  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4823508262634277  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7406158447265625  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.740617275238037  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4820241928100586  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7302746772766113  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7302756309509277  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117466926574707  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.9492826461791992
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004911661148071289
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.296966552734375e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006315708160400391
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0050356388092041016
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007099151611328125
self.buckets_partition() spend  sec:  0.005675077438354492
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172380447387695  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3932957649230957  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3932766914367676  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4835114479064941  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7655997276306152  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7656011581420898  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482764720916748  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7424511909484863  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.742452621459961  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4820685386657715  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7346129417419434  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734614372253418  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4822654724121094  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427458763122559  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427473068237305  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4821419715881348  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.731788158416748  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7317891120910645  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175856590270996  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.3616224527359009
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004891157150268555
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.630752563476562e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006358623504638672
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0050241947174072266
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007088184356689453
self.buckets_partition() spend  sec:  0.0056705474853515625
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172380447387695  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3907809257507324  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.390761375427246  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4827427864074707  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7625231742858887  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7625246047973633  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828391075134277  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7421073913574219  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7421088218688965  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4821219444274902  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7332682609558105  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7332696914672852  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4823250770568848  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7460474967956543  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.746048927307129  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482072353363037  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.731010913848877  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7310118675231934  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117516040802002  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1424607038497925
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004940986633300781
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.584426879882812e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006570816040039062
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005081653594970703
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007205009460449219
self.buckets_partition() spend  sec:  0.005748271942138672
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172380447387695  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3852300643920898  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3852105140686035  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825773239135742  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7642631530761719  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7642645835876465  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482882022857666  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7467350959777832  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7467365264892578  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482314109802246  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7345857620239258  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7345871925354004  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4823999404907227  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7435717582702637  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7435731887817383  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4821257591247559  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7326207160949707  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.732621669769287  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175694465637207  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9704411625862122
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004931211471557617
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.225440979003906e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006270408630371094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005058765411376953
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007122516632080078
self.buckets_partition() spend  sec:  0.005695343017578125
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172380447387695  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3883476257324219  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3883280754089355  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4831690788269043  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7677397727966309  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7677412033081055  GigaBytes
Max Memory Allocated: 2.0115880966186523  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4833030700683594  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427177429199219  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427191734313965  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826445579528809  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7327032089233398  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7327046394348145  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828691482543945  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7470331192016602  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7470345497131348  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826064109802246  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7352027893066406  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.735203742980957  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175551414489746  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8396478891372681
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004950761795043945
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.179115295410156e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0007090568542480469
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005094289779663086
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007306098937988281
self.buckets_partition() spend  sec:  0.005812168121337891
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172370910644531  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3863434791564941  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3863239288330078  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826092720031738  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7636609077453613  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.763662338256836  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828925132751465  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.742546558380127  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7425479888916016  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482271671295166  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734022617340088  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7340240478515625  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4823570251464844  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7440986633300781  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7441000938415527  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4820241928100586  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7316250801086426  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.731626033782959  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117466926574707  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.7545614242553711
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004912376403808594
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.606910705566406e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006318092346191406
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005040884017944336
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007098197937011719
self.buckets_partition() spend  sec:  0.005681037902832031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172375679016113  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3894195556640625  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3894004821777344  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825239181518555  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7653536796569824  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.765355110168457  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828124046325684  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7433452606201172  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7433466911315918  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4821810722351074  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7361140251159668  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7361154556274414  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.48228120803833  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7438855171203613  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.743886947631836  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482072353363037  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.732534408569336  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7325353622436523  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117516040802002  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.5969927906990051
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004906654357910156
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.702278137207031e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006301403045654297
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005036354064941406
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007106304168701172
self.buckets_partition() spend  sec:  0.005675077438354492
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172494888305664  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3906011581420898  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3905816078186035  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826197624206543  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7622895240783691  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7622909545898438  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4829034805297852  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7454638481140137  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7454652786254883  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4823732376098633  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7382588386535645  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.738260269165039  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4824800491333008  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.747856616973877  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7478580474853516  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482088565826416  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7309832572937012  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7309842109680176  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175312995910645  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6022183895111084
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004907846450805664
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.797645568847656e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00063323974609375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005038738250732422
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007092952728271484
self.buckets_partition() spend  sec:  0.005681037902832031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172375679016113  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.387129783630371  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.387110710144043  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.483088493347168  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7586712837219238  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7586727142333984  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.483281135559082  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7389392852783203  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.738940715789795  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.48274564743042  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7323942184448242  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7323956489562988  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4829487800598145  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.746716022491455  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7467174530029297  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.482679843902588  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7340197563171387  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.734020709991455  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117541790008545  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.3374577760696411
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.004950046539306641
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  9.965896606445312e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006763935089111328
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005155801773071289
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007297992706298828
self.buckets_partition() spend  sec:  0.005844831466674805
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.117236614227295  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3801064491271973  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.3800873756408691  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4826726913452148  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7605786323547363  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.760580062866211  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4828176498413086  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7393879890441895  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.739389419555664  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4822068214416504  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7345314025878906  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7345328330993652  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4822874069213867  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427854537963867  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7427868843078613  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4820771217346191  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7293167114257812  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7293176651000977  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1175198554992676  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.35892874002456665
the output layer 
self.num_batch (get_in_degree_bucketing) 6
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  6
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  94
sum(estimated_mem)
2.192714788019657
15
self.K  6
G_BUCKET_ID_list [[14, 13, 12], [3], [1, 6], [2, 11], [5, 0], [8, 9, 10], [4], [7]]
Groups_mem_list  [[185, 73, 51], [307], [189, 116], [224, 80], [232, 69], [151, 101, 36], [277], [94], [94]]
G_BUCKET_ID_list length 8
backpack scheduling spend  0.0049054622650146484
current group_mem  0.3099786043167114
current group_mem  0.30721720308065414
current group_mem  0.305373452603817
current group_mem  0.30566413700580597
current group_mem  0.3024935647845268
current group_mem  0.2903957962989807
current group_mem  0.2771906107664108
current group_mem  0.09440141916275024
batches output list generation spend  8.535385131835938e-05
self.weights_list  [0.02857142857142857, 0.15714285714285714, 0.20714285714285716, 0.2, 0.22142857142857142, 0.05714285714285714, 0.10714285714285714, 0.02142857142857143]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006453990936279297
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005033731460571289
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007116794586181641
self.buckets_partition() spend  sec:  0.00568699836730957
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1172375679016113  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.391486644744873  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.391467571258545  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4825448989868164  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7607507705688477  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7607522010803223  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4827752113342285  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7418036460876465  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.741805076599121  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4822664260864258  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7393269538879395  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.739328384399414  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4824213981628418  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.746943473815918  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7469449043273926  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

step  5
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.4820561408996582  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.730039119720459  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.7300400733947754  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.72607421875 GB
    Memory Allocated: 1.1174988746643066  GigaBytes
Max Memory Allocated: 2.0132994651794434  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.21277771890163422
epoch_time_list  [4.659335374832153, 2.225106716156006, 2.2483859062194824, 2.225100517272949, 2.2152976989746094, 2.214364767074585, 2.218284845352173, 2.2225611209869385, 2.213088274002075, 2.216224431991577, 2.2119030952453613, 2.2173616886138916, 2.2197258472442627, 2.2101733684539795, 2.2153637409210205, 2.218870162963867, 2.2162041664123535, 2.215052366256714, 2.2150163650512695, 2.3816170692443848]

loading_time list   [0.004289865493774414, 0.0056421756744384766, 0.004660844802856445, 0.004626035690307617, 0.004466533660888672, 0.004207134246826172, 0.004433870315551758, 0.005223989486694336, 0.005154848098754883, 0.005056142807006836, 0.0040645599365234375, 0.005344867706298828, 0.004541635513305664, 0.004067182540893555, 0.004158496856689453, 0.004969120025634766, 0.005068063735961914, 0.004392862319946289, 0.005306720733642578, 0.005239725112915039]

 data loader gen time  [0.08518099784851074, 0.08377838134765625, 0.08411502838134766, 0.08677077293395996, 0.07196450233459473, 0.07051801681518555, 0.07660126686096191, 0.07888412475585938, 0.07223296165466309, 0.07153439521789551, 0.07020401954650879, 0.07327604293823242, 0.07984590530395508, 0.06945157051086426, 0.07491374015808105, 0.07938241958618164, 0.07659578323364258, 0.0707099437713623, 0.07222485542297363, 0.07181715965270996]
	---backpack schedule time  [0.008632421493530273, 0.00830531120300293, 0.007404804229736328, 0.007889747619628906, 0.007344722747802734, 0.0073626041412353516, 0.007442474365234375, 0.007379770278930664, 0.007401943206787109, 0.007383108139038086, 0.007352352142333984, 0.0073277950286865234, 0.007465362548828125, 0.007355213165283203, 0.007546424865722656, 0.007328987121582031, 0.0073397159576416016, 0.007334232330322266, 0.007565736770629883, 0.007361173629760742]
	---connection_check_time_list  [0.03676271438598633, 0.036338090896606445, 0.037619829177856445, 0.03858685493469238, 0.03145599365234375, 0.030803203582763672, 0.03251338005065918, 0.035775184631347656, 0.03108501434326172, 0.031735897064208984, 0.030942678451538086, 0.03193306922912598, 0.03555440902709961, 0.030560731887817383, 0.03313088417053223, 0.03567147254943848, 0.03475475311279297, 0.031160593032836914, 0.03170418739318848, 0.0321047306060791]
	---block_gen_time_list  [0.037317514419555664, 0.03611183166503906, 0.03624868392944336, 0.03763866424560547, 0.03040170669555664, 0.02973651885986328, 0.03411507606506348, 0.033164024353027344, 0.03124094009399414, 0.029920578002929688, 0.029439210891723633, 0.03148484230041504, 0.03428840637207031, 0.029103994369506836, 0.03167295455932617, 0.03380012512207031, 0.03199267387390137, 0.029510021209716797, 0.03041243553161621, 0.02981114387512207]
training time  [4.569859981536865, 2.1319563388824463, 2.1562588214874268, 2.1295440196990967, 2.1341357231140137, 2.135436773300171, 2.133079767227173, 2.1350808143615723, 2.132293462753296, 2.136324882507324, 2.133216381072998, 2.1353447437286377, 2.130969762802124, 2.1323611736297607, 2.1318936347961426, 2.13124680519104, 2.1311872005462646, 2.1354806423187256, 2.1340718269348145, 2.301213264465332]
---feature block loading time  [0.41800618171691895, 0.4128601551055908, 0.41245579719543457, 0.4109654426574707, 0.41190481185913086, 0.41080808639526367, 0.41230010986328125, 0.41262340545654297, 0.41104650497436523, 0.41216564178466797, 0.41170597076416016, 0.4112412929534912, 0.4117863178253174, 0.412219762802124, 0.41182756423950195, 0.41515135765075684, 0.41123247146606445, 0.4129061698913574, 0.41239142417907715, 0.41132164001464844]


epoch_time avg   2.2263193130493164
loading_time avg   0.004730984568595886
 data loader gen time avg 0.07375979423522949
	---backpack schedule time avg 0.0073932260274887085
	---connection_check_time avg  0.032555386424064636
	---block_gen_time avg  0.03125591576099396
training time  2.143958553671837
---feature block loading time  0.41203953325748444
pure train time per /epoch  [4.142162561416626, 1.6247479915618896, 1.649940013885498, 1.6238384246826172, 1.6272754669189453, 1.630143404006958, 1.6267037391662598, 1.6279590129852295, 1.6267461776733398, 1.6300270557403564, 1.6275291442871094, 1.630059003829956, 1.625251054763794, 1.6263203620910645, 1.6262845993041992, 1.6219937801361084, 1.6253104209899902, 1.628852128982544, 1.6280517578125, 1.7954366207122803]
pure train time average  1.636928362004897

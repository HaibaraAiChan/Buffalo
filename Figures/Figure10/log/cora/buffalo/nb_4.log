main start at this time 1733007564.4921627
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
success----------------------------------------
# Nodes: 2708
# Edges: 10556
# Train: 140
# Val: 500
# Test: 2068
# Classes: 7

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005388975143432617
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  0.0003998279571533203
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0009980201721191406
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005843162536621094
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.008257150650024414
self.buckets_partition() spend  sec:  0.006851673126220703
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 0.90771484375 GB
    Memory Allocated: 0.3746180534362793  GigaBytes
Max Memory Allocated: 0.3746180534362793  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.33740234375 GB
    Memory Allocated: 0.772984504699707  GigaBytes
Max Memory Allocated: 0.7747311592102051  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.33740234375 GB
    Memory Allocated: 0.7729864120483398  GigaBytes
Max Memory Allocated: 0.7747311592102051  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.80224609375 GB
    Memory Allocated: 0.755469799041748  GigaBytes
Max Memory Allocated: 1.1619043350219727  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 1.80615234375 GB
    Memory Allocated: 1.158156394958496  GigaBytes
Max Memory Allocated: 1.1619043350219727  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 1.80615234375 GB
    Memory Allocated: 1.158158302307129  GigaBytes
Max Memory Allocated: 1.1619043350219727  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.05615234375 GB
    Memory Allocated: 0.7558088302612305  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.05615234375 GB
    Memory Allocated: 1.1289100646972656  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.05615234375 GB
    Memory Allocated: 1.128911018371582  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.05615234375 GB
    Memory Allocated: 0.7548046112060547  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.07177734375 GB
    Memory Allocated: 1.1184496879577637  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.07177734375 GB
    Memory Allocated: 1.11845064163208  GigaBytes
Max Memory Allocated: 1.4066534042358398  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.1194243431091309  GigaBytes
Max Memory Allocated: 1.8506979942321777  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.5866767168045044
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005377769470214844
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  7.104873657226562e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006988048553466797
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005492448806762695
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007172107696533203
self.buckets_partition() spend  sec:  0.006199836730957031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 1.8506979942321777  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.5128145217895508  GigaBytes
Max Memory Allocated: 1.8506979942321777  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.5127825736999512  GigaBytes
Max Memory Allocated: 1.8506979942321777  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.4846301078796387  GigaBytes
Max Memory Allocated: 1.8937811851501465  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.89125394821167  GigaBytes
Max Memory Allocated: 1.8937811851501465  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.70849609375 GB
    Memory Allocated: 1.8912558555603027  GigaBytes
Max Memory Allocated: 1.8937811851501465  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484969139099121  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8614211082458496  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.861422061920166  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846954345703125  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8468828201293945  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.846883773803711  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201539039611816  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.6411346197128296
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005368709564208984
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.723403930664062e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006387233734130859
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005472898483276367
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007081508636474609
self.buckets_partition() spend  sec:  0.006119966506958008
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5149202346801758  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5148882865905762  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846291542053223  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.883653163909912  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.883655071258545  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4849681854248047  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8568286895751953  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8568296432495117  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484696388244629  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8539800643920898  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8539810180664062  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201558113098145  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.575690746307373
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005382061004638672
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  7.605552673339844e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006754398345947266
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.00550389289855957
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007233381271362305
self.buckets_partition() spend  sec:  0.006237983703613281
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5095210075378418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5094890594482422  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846034049987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8881072998046875  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8881092071533203  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484570026397705  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8586554527282715  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.858656406402588  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4850010871887207  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8499751091003418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8499760627746582  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1204605102539062  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.523369312286377
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005373716354370117
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  5.984306335449219e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006422996520996094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005475759506225586
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.0070726871490478516
self.buckets_partition() spend  sec:  0.00612640380859375
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201000213623047  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5087265968322754  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5086946487426758  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4857125282287598  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8873720169067383  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.887373924255371  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484940528869629  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8587818145751953  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8587827682495117  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.48577880859375  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8536548614501953  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8536558151245117  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.120460033416748  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4849376678466797
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.0053882598876953125
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.699562072753906e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.000690460205078125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0054929256439208984
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007152557373046875
self.buckets_partition() spend  sec:  0.0061931610107421875
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1202335357666016  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5117664337158203  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5117344856262207  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4862470626831055  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.890547752380371  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.890549659729004  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4856152534484863  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8620610237121582  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8620619773864746  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4863147735595703  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8657207489013672  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8657217025756836  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.120462417602539  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.4297136068344116
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005377531051635742
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.079673767089844e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006229877471923828
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005476713180541992
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007057905197143555
self.buckets_partition() spend  sec:  0.006108283996582031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5143728256225586  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.514340877532959  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846186637878418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8864622116088867  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8864641189575195  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4849696159362793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8573102951049805  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8573112487792969  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846863746643066  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8578524589538574  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8578534126281738  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201457977294922  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.339356541633606
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005400419235229492
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.556510925292969e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006229877471923828
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005507707595825195
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007090091705322266
self.buckets_partition() spend  sec:  0.006140232086181641
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5123238563537598  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5122919082641602  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4845881462097168  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.886404037475586  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8864059448242188  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484969139099121  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8624415397644043  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8624424934387207  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484654426574707  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.847381591796875  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8473825454711914  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201138496398926  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1745318174362183
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.00539398193359375
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  5.91278076171875e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006299018859863281
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005497455596923828
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007084369659423828
self.buckets_partition() spend  sec:  0.006135702133178711
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.514120101928711  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5140881538391113  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484614372253418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.886369228363037  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.88637113571167  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846439361572266  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8647737503051758  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8647747039794922  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4850006103515625  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8560404777526855  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.856041431427002  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.120460033416748  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 4.94467306137085
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.0053975582122802734
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.747245788574219e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006453990936279297
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005508899688720703
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007134914398193359
self.buckets_partition() spend  sec:  0.0061664581298828125
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201748847961426  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5164413452148438  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5164093971252441  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846410751342773  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8884601593017578  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8884620666503906  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.484968662261963  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8531694412231445  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.853170394897461  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4847073554992676  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.856860637664795  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8568615913391113  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1201667785644531  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.7575006484985352
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005368947982788086
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.794929504394531e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006239414215087891
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005536556243896484
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007120847702026367
self.buckets_partition() spend  sec:  0.006169557571411133
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5123414993286133  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5123095512390137  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4844965934753418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8888182640075684  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8888201713562012  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4849681854248047  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8537564277648926  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.853757381439209  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4840078353881836  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8530011177062988  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8530020713806152  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1194672584533691  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2411024570465088
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005366325378417969
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.341934204101562e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006356239318847656
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005475044250488281
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007084369659423828
self.buckets_partition() spend  sec:  0.006123781204223633
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5114340782165527  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.5114021301269531  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4846620559692383  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.889206886291504  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8892087936401367  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4849681854248047  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8590655326843262  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.8590664863586426  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77099609375 GB
    Memory Allocated: 1.4840397834777832  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8525586128234863  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8525595664978027  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1194992065429688  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.2425580024719238
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005356550216674805
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.127357482910156e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006377696990966797
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005468606948852539
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007063865661621094
self.buckets_partition() spend  sec:  0.0061147212982177734
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5139479637145996  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.513916015625  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4845662117004395  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8851966857910156  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8851985931396484  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.484969139099121  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8622403144836426  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.862241268157959  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4846324920654297  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8482308387756348  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8482317924499512  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1200919151306152  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1786699295043945
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005364418029785156
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.079673767089844e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006172657012939453
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005463600158691406
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007046937942504883
self.buckets_partition() spend  sec:  0.006089448928833008
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.513798713684082  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5137667655944824  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4845662117004395  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8885712623596191  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.888573169708252  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4849696159362793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.861727237701416  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8617281913757324  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4839816093444824  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8527064323425293  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8527073860168457  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.119441032409668  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 1.1051051616668701
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005393266677856445
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.461143493652344e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006232261657714844
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005500316619873047
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007071018218994141
self.buckets_partition() spend  sec:  0.006131887435913086
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5116753578186035  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.511643409729004  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4845128059387207  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.890716552734375  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8907184600830078  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4849681854248047  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8591766357421875  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.859177589416504  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4840397834777832  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8537330627441406  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.853734016418457  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1194992065429688  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.9910967350006104
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005439281463623047
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.866455078125e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006556510925292969
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005549192428588867
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007179975509643555
self.buckets_partition() spend  sec:  0.006217002868652344
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5112800598144531  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5112481117248535  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4844059944152832  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.884613037109375  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8846149444580078  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.484969139099121  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8631577491760254  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8631587028503418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4840397834777832  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8489527702331543  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8489537239074707  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1194992065429688  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.8813998699188232
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005349159240722656
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.461143493652344e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006334781646728516
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.0054547786712646484
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007035255432128906
self.buckets_partition() spend  sec:  0.006097555160522461
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5148530006408691  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5148210525512695  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.484571933746338  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8894758224487305  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8894777297973633  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4849696159362793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8620409965515137  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.86204195022583  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4840669631958008  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8537378311157227  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.853738784790039  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1195263862609863  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.732364296913147
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005375385284423828
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.699562072753906e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006306171417236328
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005540609359741211
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007133960723876953
self.buckets_partition() spend  sec:  0.0061798095703125
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5113525390625  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5113205909729004  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4844474792480469  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8839707374572754  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8839726448059082  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.484969139099121  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8607454299926758  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8607463836669922  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4840130805969238  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8489274978637695  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.848928451538086  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1194725036621094  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.6247671246528625
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.0053598880767822266
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.413459777832031e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0006504058837890625
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005462646484375
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007078409194946289
self.buckets_partition() spend  sec:  0.0061261653900146484
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1203455924987793  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5126299858093262  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5125980377197266  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4845447540283203  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8893976211547852  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.889399528503418  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.48455810546875  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8646373748779297  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.864638328552246  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4839320182800293  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.839890956878662  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8398919105529785  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1193904876708984  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.5614430904388428
the output layer 
self.num_batch (get_in_degree_bucketing) 4
get_in_degree_bucketing dst global nid length 140
len(bkt)  20
len(bkt)  25
len(bkt)  26
len(bkt)  22
len(bkt)  15
len(bkt)  11
len(bkt)  4
len(bkt)  3
len(bkt)  4
len(bkt)  3
len(bkt)  1
len(bkt)  2
len(bkt)  1
len(bkt)  1
len(bkt)  2
total indegree bucketing result ,  140
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  15
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  4
memory_constraint:  7.5
grouping float:  the grouping_fanout_cora called successfully
 enter split_cora function
the last batch value is  224
sum(estimated_mem)
2.192714788019657
15
self.K  4
G_BUCKET_ID_list [[1, 6, 9, 7], [3, 13, 0, 12], [5, 8, 11, 10], [4, 14], [2]]
Groups_mem_list  [[189, 116, 101, 94], [307, 73, 69, 51], [232, 151, 80, 36], [277, 185], [224], [224]]
G_BUCKET_ID_list length 5
backpack scheduling spend  0.005394935607910156
current group_mem  0.5014110803604126
current group_mem  0.5018687322735786
current group_mem  0.502223052084446
current group_mem  0.46223312616348267
current group_mem  0.22497879713773727
batches output list generation spend  6.389617919921875e-05
self.weights_list  [0.25, 0.3142857142857143, 0.12857142857142856, 0.12142857142857143, 0.18571428571428572]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00067901611328125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.005501508712768555
self.has_zero_indegree_seeds  False
num_output  140
self.output_nids  140
output nodes length match
global output equals  True
partition total batch output list spend :  0.007190704345703125
self.buckets_partition() spend  sec:  0.006189107894897461
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1201696395874023  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.512585163116455  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.5125532150268555  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4849343299865723  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8883509635925293  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.888352870941162  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4841623306274414  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8585071563720703  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8585081100463867  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.4840078353881836  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8503155708312988  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.8503165245056152  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 2.77294921875 GB
    Memory Allocated: 1.1194672584533691  GigaBytes
Max Memory Allocated: 2.1397509574890137  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 0.4215218126773834
epoch_time_list  [3.1723859310150146, 1.8028006553649902, 1.7877366542816162, 1.762162446975708, 1.7895710468292236, 1.792809247970581, 1.798351526260376, 1.8016951084136963, 1.7953622341156006, 1.8039753437042236, 1.7922298908233643, 1.793048620223999, 1.7636256217956543, 1.7600221633911133, 1.798915147781372, 1.7786285877227783, 1.7610864639282227, 1.7894940376281738, 1.7621519565582275, 1.779616117477417]

loading_time list   [0.004334926605224609, 0.004547834396362305, 0.0043010711669921875, 0.0037097930908203125, 0.003732919692993164, 0.0035581588745117188, 0.003869771957397461, 0.003657817840576172, 0.0034482479095458984, 0.004374980926513672, 0.004349946975708008, 0.003603219985961914, 0.004250764846801758, 0.0037343502044677734, 0.003717184066772461, 0.003945827484130859, 0.003719329833984375, 0.004125833511352539, 0.003532886505126953, 0.004395723342895508]

 data loader gen time  [0.05242776870727539, 0.0557706356048584, 0.052640438079833984, 0.04855823516845703, 0.048006296157836914, 0.04750180244445801, 0.05177927017211914, 0.05434370040893555, 0.05119967460632324, 0.05870461463928223, 0.046422481536865234, 0.05112862586975098, 0.048752784729003906, 0.05038309097290039, 0.0549769401550293, 0.048601388931274414, 0.048929452896118164, 0.04756450653076172, 0.04925394058227539, 0.05042266845703125]
	---backpack schedule time  [0.0085601806640625, 0.0074367523193359375, 0.007324695587158203, 0.007483005523681641, 0.0073089599609375, 0.007392406463623047, 0.007294893264770508, 0.007337093353271484, 0.007321596145629883, 0.007399082183837891, 0.007362842559814453, 0.007326364517211914, 0.007300853729248047, 0.007275104522705078, 0.007307291030883789, 0.00744175910949707, 0.0072743892669677734, 0.007381916046142578, 0.0073277950286865234, 0.007437944412231445]
	---connection_check_time_list  [0.020247459411621094, 0.022966384887695312, 0.02097773551940918, 0.020536422729492188, 0.019445419311523438, 0.019408702850341797, 0.021271944046020508, 0.021997928619384766, 0.021355152130126953, 0.025077342987060547, 0.01920914649963379, 0.020375490188598633, 0.019937753677368164, 0.020035266876220703, 0.022495269775390625, 0.019910812377929688, 0.019981861114501953, 0.019655466079711914, 0.01984548568725586, 0.020207643508911133]
	---block_gen_time_list  [0.021525144577026367, 0.023045778274536133, 0.021966218948364258, 0.01825881004333496, 0.01909613609313965, 0.018402814865112305, 0.020924806594848633, 0.0225679874420166, 0.020270586013793945, 0.023855924606323242, 0.017677783966064453, 0.021151065826416016, 0.019310712814331055, 0.020753145217895508, 0.02291107177734375, 0.019040346145629883, 0.019504070281982422, 0.0183258056640625, 0.01995062828063965, 0.020389318466186523]
training time  [3.1156187057495117, 1.7396290302276611, 1.7284164428710938, 1.706617832183838, 1.7349770069122314, 1.73868989944458, 1.7397170066833496, 1.7406535148620605, 1.7375805377960205, 1.7384757995605469, 1.739030361175537, 1.7350504398345947, 1.7081294059753418, 1.7031102180480957, 1.737095832824707, 1.7231996059417725, 1.7054502964019775, 1.7346317768096924, 1.7063624858856201, 1.7217235565185547]
---feature block loading time  [0.26304197311401367, 0.256894588470459, 0.25694823265075684, 0.25582337379455566, 0.2541782855987549, 0.2564125061035156, 0.2555868625640869, 0.25686025619506836, 0.25507545471191406, 0.2560603618621826, 0.2550501823425293, 0.2545440196990967, 0.25568056106567383, 0.2538337707519531, 0.25458812713623047, 0.2560124397277832, 0.2559373378753662, 0.2545926570892334, 0.25576186180114746, 0.25484681129455566]


epoch_time avg   1.7850364446640015
loading_time avg   0.0038760602474212646
 data loader gen time avg 0.050498202443122864
	---backpack schedule time avg 0.0073431432247161865
	---connection_check_time avg  0.02063816785812378
	---block_gen_time avg  0.020258262753486633
training time  1.7277423590421677
---feature block loading time  0.25531384348869324
pure train time per /epoch  [2.8443892002105713, 1.3905034065246582, 1.3784172534942627, 1.3584723472595215, 1.387641429901123, 1.3892383575439453, 1.391089677810669, 1.3908088207244873, 1.3896119594573975, 1.3894715309143066, 1.3920700550079346, 1.3883848190307617, 1.360158920288086, 1.355600357055664, 1.388946533203125, 1.3749089241027832, 1.3566596508026123, 1.3875377178192139, 1.3582379817962646, 1.373164176940918]
pure train time average  1.3783531329211067

main start at this time 1733009307.652311
-----------------------------------------before load data 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

----------------------------------------start of run function 
 Nvidia-smi: 0.3560791015625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4706454277038574
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0007138252258300781
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007930517196655273
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.47159290313720703
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6189138889312744
self.buckets_partition() spend  sec:  0.47954463958740234
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 0.63818359375 GB
    Memory Allocated: 0.10211944580078125  GigaBytes
Max Memory Allocated: 0.10211944580078125  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 17.34326171875 GB
    Memory Allocated: 15.411481857299805  GigaBytes
Max Memory Allocated: 16.19836664199829  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 17.34326171875 GB
    Memory Allocated: 15.413891792297363  GigaBytes
Max Memory Allocated: 16.19836664199829  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 18.00537109375 GB
    Memory Allocated: 0.1646890640258789  GigaBytes
Max Memory Allocated: 16.19836664199829  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 20.98779296875 GB
    Memory Allocated: 15.556986808776855  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 20.98779296875 GB
    Memory Allocated: 15.562083721160889  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 21.61474609375 GB
    Memory Allocated: 0.16986942291259766  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 21.61669921875 GB
    Memory Allocated: 14.738566398620605  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 21.61669921875 GB
    Memory Allocated: 14.742358207702637  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.20849609375 GB
    Memory Allocated: 0.1643376350402832  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.21826171875 GB
    Memory Allocated: 14.32664155960083  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.21826171875 GB
    Memory Allocated: 14.3283052444458  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.21826171875 GB
    Memory Allocated: 0.16166162490844727  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 10.310758113861084  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 10.31217908859253  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 0.2007889747619629  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.77663254737854
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.450040340423584
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00037670135498046875
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007565736770629883
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.4505486488342285
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6014659404754639
self.buckets_partition() spend  sec:  0.45813894271850586
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 0.18813562393188477  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 15.483044624328613  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 15.482770919799805  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 0.23131799697875977  GigaBytes
Max Memory Allocated: 16.425053596496582  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 15.651937007904053  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 15.657033920288086  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 0.23652362823486328  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 14.807927131652832  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 14.811718940734863  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 0.23054742813110352  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 14.390806674957275  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22021484375 GB
    Memory Allocated: 14.392470359802246  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22216796875 GB
    Memory Allocated: 0.22836923599243164  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.365426063537598  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.366834163665771  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20100116729736328  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.191472291946411
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.42670202255249023
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00046563148498535156
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0077972412109375
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.42728400230407715
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5754921436309814
self.buckets_partition() spend  sec:  0.4351050853729248
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18809032440185547  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.48524284362793  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.485105514526367  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312755584716797  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.639072895050049  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.644169807434082  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23629426956176758  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.847568035125732  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.851359844207764  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23067998886108398  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.34289264678955  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.344396114349365  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2286243438720703  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.368879795074463  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.370285034179688  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20125436782836914  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.6062698364257812
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.3072834014892578
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00043702125549316406
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008507966995239258
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.3078737258911133
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.45572996139526367
self.buckets_partition() spend  sec:  0.3164057731628418
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1882171630859375  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.492105484008789  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.491889476776123  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23118829727172852  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.645519733428955  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.65045976638794  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23615455627441406  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.796507358551025  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.800299167633057  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2305469512939453  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.384163856506348  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.385716915130615  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2286205291748047  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.37962007522583  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.381009578704834  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20118379592895508  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.0818521976470947
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4864788055419922
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004792213439941406
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008778095245361328
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.4871087074279785
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6369130611419678
self.buckets_partition() spend  sec:  0.4959125518798828
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1880354881286621  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.48048734664917  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.480274677276611  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312297821044922  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.643688201904297  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.64878511428833  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2365107536315918  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.872794151306152  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.876891613006592  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23071622848510742  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.387821197509766  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.389484882354736  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22873306274414062  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.382761478424072  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.384242057800293  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2012801170349121  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.066500425338745
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.43761372566223145
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004208087921142578
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007533550262451172
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.43816375732421875
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5854723453521729
self.buckets_partition() spend  sec:  0.4457221031188965
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1880030632019043  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.470738887786865  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.470571994781494  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23121881484985352  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.642468452453613  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.647565364837646  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23645925521850586  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.820230484008789  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.82402229309082  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2307147979736328  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.339463710784912  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.34093189239502  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22863006591796875  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.382526397705078  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.38390302658081  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20127630233764648  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.066805362701416
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4749598503112793
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004177093505859375
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008484601974487305
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.47551798820495605
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6241476535797119
self.buckets_partition() spend  sec:  0.4840250015258789
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1879863739013672  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.48032283782959  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.480203628540039  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23139476776123047  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.642445087432861  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.647279739379883  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2359752655029297  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.833566188812256  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.837357997894287  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2307147979736328  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.391873836517334  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.393537521362305  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22852706909179688  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.372085571289062  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.37348985671997  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20113849639892578  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 3.0274953842163086
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.429455041885376
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00042366981506347656
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007586240768432617
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.43007850646972656
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5775225162506104
self.buckets_partition() spend  sec:  0.43768739700317383
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18813228607177734  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.466935634613037  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.466701984405518  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23121213912963867  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.647884368896484  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.652981281280518  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23639345169067383  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.796528339385986  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.800320148468018  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044729232788086  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.38073444366455  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.382398128509521  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22877740859985352  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.38676404953003  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.38815975189209  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20122051239013672  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.959014892578125
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.307711124420166
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00046825408935546875
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00805211067199707
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.30834436416625977
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.4577038288116455
self.buckets_partition() spend  sec:  0.3164217472076416
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18818950653076172  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.494240283966064  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.493969917297363  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23122119903564453  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.635539054870605  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.640410900115967  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23622560501098633  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.842674732208252  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.846466541290283  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044824600219727  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.386650562286377  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.388264656066895  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22870540618896484  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.383210182189941  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.384623050689697  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20129871368408203  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.8676280975341797
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.45748186111450195
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00040721893310546875
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008009910583496094
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.45801353454589844
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6056802272796631
self.buckets_partition() spend  sec:  0.4660522937774658
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18813323974609375  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.492356300354004  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.49209213256836  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312912940979004  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.63756275177002  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.642659664154053  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23642444610595703  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.802183628082275  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.805975437164307  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044776916503906  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.384592533111572  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.386256217956543  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2286357879638672  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.388551712036133  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.389949321746826  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20124578475952148  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.766777276992798
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.446974515914917
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004291534423828125
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008897542953491211
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.44762349128723145
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5963499546051025
self.buckets_partition() spend  sec:  0.4565455913543701
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18796062469482422  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.47479248046875  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.474579334259033  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312922477722168  GigaBytes
Max Memory Allocated: 16.519647121429443  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.652077674865723  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.657174587249756  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23646211624145508  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.802735328674316  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.806527137756348  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044729232788086  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.350364208221436  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.352027893066406  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22882556915283203  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.380535125732422  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.381937503814697  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20126724243164062  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.6722238063812256
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4288647174835205
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004296302795410156
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.00837850570678711
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.42942237854003906
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5789632797241211
self.buckets_partition() spend  sec:  0.437824010848999
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18813371658325195  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.524081707000732  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.523866653442383  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23117589950561523  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.628571033477783  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.633667945861816  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23643970489501953  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.841638565063477  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.845430374145508  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044872283935547  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.34797477722168  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.34945821762085  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22851276397705078  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.377633571624756  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.379077434539795  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20114755630493164  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.606044054031372
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.47229528427124023
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004343986511230469
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007672548294067383
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.47286534309387207
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6199305057525635
self.buckets_partition() spend  sec:  0.4805624485015869
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18810749053955078  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.48525094985962  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.485068798065186  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23115777969360352  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.621327877044678  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.626424789428711  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2364521026611328  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.79990816116333  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.80400562286377  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2307138442993164  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.347753524780273  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.349256992340088  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22870588302612305  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.37440299987793  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.375809669494629  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2012767791748047  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.559502124786377
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.3072543144226074
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00041365623474121094
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.008595705032348633
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.3078031539916992
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.4579911231994629
self.buckets_partition() spend  sec:  0.31642723083496094
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1880326271057129  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.483444213867188  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.483222007751465  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312626838684082  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.652439594268799  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.657369136810303  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23644351959228516  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.798384666442871  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.80248212814331  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23071622848510742  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.34541130065918  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.346889019012451  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22861909866333008  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.374006271362305  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.375410079956055  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20123863220214844  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.4726979732513428
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4842250347137451
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00043272972106933594
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007434844970703125
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.484788179397583
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6212611198425293
self.buckets_partition() spend  sec:  0.4922478199005127
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1880955696105957  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.497762680053711  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.497513771057129  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23123979568481445  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.647876262664795  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.652973175048828  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2364034652709961  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.799223899841309  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.80301570892334  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044824600219727  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.351307392120361  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.352971076965332  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22873353958129883  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.380376815795898  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.38185739517212  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20119762420654297  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.3968634605407715
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4450347423553467
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004181861877441406
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007025480270385742
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.44557714462280273
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5817208290100098
self.buckets_partition() spend  sec:  0.4526252746582031
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18807125091552734  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.482897281646729  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.482629299163818  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2312469482421875  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.650353908538818  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.65530014038086  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2364187240600586  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.842653274536133  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.846445083618164  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044824600219727  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.382383346557617  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.383902072906494  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22861719131469727  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.380012512207031  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.381399154663086  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20120716094970703  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.321197509765625
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.47055721282958984
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0004184246063232422
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007210731506347656
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.47110748291015625
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.6067414283752441
self.buckets_partition() spend  sec:  0.47834277153015137
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18812799453735352  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.470737934112549  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.470501899719238  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23132848739624023  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.643632888793945  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.648729801177979  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23651456832885742  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.851651191711426  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.855443000793457  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044919967651367  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.332225799560547  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.333889484405518  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2289133071899414  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.388688087463379  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.390058040618896  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2013564109802246  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.2494373321533203
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.4294149875640869
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.0003504753112792969
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.006543874740600586
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.43003058433532715
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5652241706848145
self.buckets_partition() spend  sec:  0.4365978240966797
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18807029724121094  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.487368106842041  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.487175941467285  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23135614395141602  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.636537551879883  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.641619205474854  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23641061782836914  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.853848934173584  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.857640743255615  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23044824600219727  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.37544870376587  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.376919746398926  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22861528396606445  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.386192798614502  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.387653827667236  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20128631591796875  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.1930673122406006
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.30327916145324707
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00043010711669921875
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.0068972110748291016
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.3038156032562256
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.45119524002075195
self.buckets_partition() spend  sec:  0.31073641777038574
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.18822002410888672  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.494554996490479  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.49429178237915  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2308192253112793  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.641185760498047  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.64628267288208  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2359452247619629  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.827162265777588  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.83095407485962  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2300257682800293  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.337835788726807  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.339312553405762  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22775888442993164  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.393555641174316  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.39493465423584  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20049190521240234  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.145827531814575
the output layer 
self.num_batch (get_in_degree_bucketing) 5
get_in_degree_bucketing dst global nid length 90941
len(bkt)  13428
len(bkt)  11706
len(bkt)  9277
len(bkt)  7320
len(bkt)  6222
len(bkt)  4868
len(bkt)  4045
len(bkt)  3472
len(bkt)  2976
len(bkt)  2599
len(bkt)  2203
len(bkt)  1937
len(bkt)  1656
len(bkt)  1434
len(bkt)  1289
len(bkt)  1111
len(bkt)  1030
len(bkt)  948
len(bkt)  836
len(bkt)  807
len(bkt)  717
len(bkt)  556
len(bkt)  519
len(bkt)  525
len(bkt)  9460
total indegree bucketing result ,  90941
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  5
type of fanout_dst_nids  <class 'torch.Tensor'>
self.K  5
G_BUCKET_ID_list [[6, 8, 9, 12, 13], [4, 3, 11, 0, 14], [2, 1, 19, 20, 23, 22], [7, 15, 16, 17, 18, 21], [5, 10]]
Groups_mem_list  [[1881, 1805, 1779, 1591, 1544], [1966, 1929, 1683, 1514, 1508], [1901, 1785, 1326, 1281, 1159, 1148], [1853, 1418, 1415, 1370, 1351, 1144], [1921, 1685]]
G_BUCKET_ID_list length 5
backpack scheduling spend time (sec) 0.44154977798461914
len(g_bucket_nids_list)  5
len(local_split_batches_nid_list)  5
current group_mem  8.602024495220023
current group_mem  8.603722486275078
current group_mem  8.602923641502407
current group_mem  8.554943147503527
current group_mem  3.607020921101766
batches output list generation spend  0.00043487548828125
self.weights_list  [0.16056564145984759, 0.35284415170275235, 0.27977479904553504, 0.10825700179237088, 0.09855840599949418]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.007871627807617188
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  0.4421117305755615
self.has_zero_indegree_seeds  False
num_output  90941
self.output_nids  90941
output nodes length match
global output equals  True
partition total batch output list spend :  0.5788798332214355
self.buckets_partition() spend  sec:  0.45000648498535156
step  0
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.1879425048828125  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.464405536651611  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.46425724029541  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  1
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23120689392089844  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.64329195022583  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 15.6483736038208  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  2
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.23609495162963867  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.836182117462158  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.83997392654419  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  3
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.2307138442993164  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.388251781463623  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 14.389915466308594  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

step  4
----------------------------------------before batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.22858381271362305  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after batch_pred = model(blocks, batch_inputs)
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.379286766052246  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after loss function
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 10.380707263946533  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------after optimizer
 Nvidia-smi: 22.22412109375 GB
    Memory Allocated: 0.20122575759887695  GigaBytes
Max Memory Allocated: 16.52056646347046  GigaBytes

----------------------------------------------------------pseudo_mini_loss sum 2.0858662128448486
epoch_time_list  [6.5286054611206055, 5.4167187213897705, 5.34577751159668, 5.274720191955566, 5.460599422454834, 5.383450031280518, 5.416323184967041, 5.358709096908569, 5.271557807922363, 5.419835329055786, 5.404207706451416, 5.42424201965332, 5.453040599822998, 5.320261716842651, 5.422172784805298, 5.415010929107666, 5.440272092819214, 5.358587980270386, 5.265933036804199, 5.423028945922852]

loading_time list   [0.02532172203063965, 0.06137824058532715, 0.03970217704772949, 0.028201580047607422, 0.055310726165771484, 0.0370328426361084, 0.028976917266845703, 0.027005672454833984, 0.04291820526123047, 0.03978323936462402, 0.02523183822631836, 0.0431971549987793, 0.041417598724365234, 0.026282548904418945, 0.04424238204956055, 0.031467437744140625, 0.041185617446899414, 0.030247211456298828, 0.024309396743774414, 0.04371929168701172]

 data loader gen time  [1.4849119186401367, 1.497321605682373, 1.4387757778167725, 1.354734182357788, 1.5264041423797607, 1.4509201049804688, 1.494368314743042, 1.4325025081634521, 1.3222951889038086, 1.473381757736206, 1.460204839706421, 1.4632315635681152, 1.491288185119629, 1.3549470901489258, 1.4615342617034912, 1.400007963180542, 1.473341703414917, 1.4008715152740479, 1.2981712818145752, 1.4401893615722656]
	---backpack schedule time  [0.6315212249755859, 0.6134605407714844, 0.5883316993713379, 0.4656033515930176, 0.6500282287597656, 0.5952000617980957, 0.6344847679138184, 0.5875585079193115, 0.46961069107055664, 0.6154136657714844, 0.6066629886627197, 0.588569164276123, 0.6324665546417236, 0.46796369552612305, 0.63376784324646, 0.5902974605560303, 0.6169226169586182, 0.5748229026794434, 0.457627534866333, 0.5852866172790527]
	---connection_check_time_list  [0.43666934967041016, 0.4476814270019531, 0.43904662132263184, 0.4741990566253662, 0.44927191734313965, 0.4381978511810303, 0.44972801208496094, 0.44207119941711426, 0.4405679702758789, 0.4434812068939209, 0.455014705657959, 0.456571102142334, 0.44235920906066895, 0.45850038528442383, 0.4424588680267334, 0.4536406993865967, 0.4412059783935547, 0.43170595169067383, 0.44375085830688477, 0.44451236724853516]
	---block_gen_time_list  [0.35632872581481934, 0.373676061630249, 0.3516042232513428, 0.35166382789611816, 0.3665194511413574, 0.3572089672088623, 0.3492906093597412, 0.3429391384124756, 0.3502345085144043, 0.3542165756225586, 0.3369872570037842, 0.35735273361206055, 0.35605335235595703, 0.36670875549316406, 0.3327980041503906, 0.3031477928161621, 0.3631563186645508, 0.3419182300567627, 0.3441348075866699, 0.3559696674346924]
training time  [5.018365144729614, 3.8561887741088867, 3.8652472496032715, 3.8893542289733887, 3.8764209747314453, 3.8933117389678955, 3.8905458450317383, 3.8969967365264893, 3.9039924144744873, 3.901942729949951, 3.916027307510376, 3.9153859615325928, 3.918057918548584, 3.9367024898529053, 3.914064407348633, 3.981135606765747, 3.9234964847564697, 3.925248861312866, 3.941162109375, 3.9367611408233643]
---feature block loading time  [0.9120054244995117, 1.1420185565948486, 1.1431756019592285, 1.158775806427002, 1.1383740901947021, 1.1562600135803223, 1.1423101425170898, 1.1440110206604004, 1.1512129306793213, 1.1514599323272705, 1.1585180759429932, 1.158491611480713, 1.1645679473876953, 1.1647825241088867, 1.1508939266204834, 1.2161586284637451, 1.157825231552124, 1.1536612510681152, 1.1691534519195557, 1.1626968383789062]


epoch_time avg   5.3898270428180695
loading_time avg   0.03639550507068634
 data loader gen time avg 1.4339787364006042
	---backpack schedule time avg 0.5816677063703537
	---connection_check_time avg  0.4458148926496506
	---block_gen_time avg  0.3486647605895996
training time  3.916953295469284
---feature block loading time  1.1587736010551453
pure train time per /epoch  [4.100858688354492, 2.476898193359375, 2.4849956035614014, 2.4938015937805176, 2.4976837635040283, 2.498776912689209, 2.5039687156677246, 2.5131585597991943, 2.513101816177368, 2.510986089706421, 2.517735242843628, 2.5172133445739746, 2.514059543609619, 2.5307676792144775, 2.5233376026153564, 2.5244176387786865, 2.524670362472534, 2.5310730934143066, 2.5252702236175537, 2.533679246902466]
pure train time average  2.5161000840804157
